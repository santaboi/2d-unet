{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19a9569e9d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader , Dataset \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from cfg import *\n",
    "torch.manual_seed(6666)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use constantpad2d() to resize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torchvision.transforms as T \n",
    "from torchvision.transforms.functional import hflip , vflip\n",
    "\n",
    "class Total_dataset(Dataset):\n",
    "    def __init__(self, inputs: list, targets: list, transform=None) -> None:\n",
    "        super().__init__()\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    def __getitem__(self , index : int  , transform = None):\n",
    "        #是否有辦法用path + file name取用data\n",
    "        input_img = self.inputs[index]\n",
    "        target_img = self.targets[index]\n",
    "        \n",
    "        x_data , y_data = cv2.imread(input_img) , cv2.imread(target_img)\n",
    "        \n",
    "        #print(x_data.shape)\n",
    "        #print(y_data.shape)\n",
    "        \n",
    "        \n",
    "        #resize\n",
    "        if x_data.shape != (512 , 512 , 3) :\n",
    "            x_data = cv2.resize(x_data , (512 , 512))\n",
    "        if y_data.shape != (512 , 512 , 3) :\n",
    "            y_data = cv2.resize(y_data , (512 ,512))\n",
    "\n",
    "        # turn to torch (typecasting)\n",
    "\n",
    "        #data augmentaion if true\n",
    "        if self.transform != None : \n",
    "            #x_data , y_data = self.transform(x_data , y_data)\n",
    "            for t in self.transform :\n",
    "                if t == \"T.Normalize([1 , 1 , 1] , [1 , 1 , 1])\" :\n",
    "                    \"\"\"\n",
    "                    x y mean0 tensor(0.2302) tensor(0.0111)\n",
    "                    x y mean1 tensor(0.1842) tensor(0.0111)\n",
    "                    x y mean2 tensor(0.1870) tensor(0.0111)\n",
    "                    x y std0 tensor(0.2334) tensor(0.0737)\n",
    "                    x y std01 tensor(0.1999) tensor(0.0737)\n",
    "                    x y std2 tensor(0.1987) tensor(0.0737)\n",
    "                    \"\"\"\n",
    "                    #normalized with whole dataset mean and std (not batchwise)\n",
    "                    img_normalize = T.Normalize([0.2302 , 0.1842 , 0.1870] , [0.2334 , 0.1999 , 0.1987])\n",
    "                    x_data = img_normalize(x_data)\n",
    "                    label_normalize = T.Normalize([0.0111 , 0.0111 , 0.0111 ] , [0.0737 , 0.0737 , 0.0737]) \n",
    "                    y_data = label_normalize(y_data)\n",
    "                elif t == \"T.RandomHorizontalFlip(p=1)\" :\n",
    "                    if torch.rand(1)< 0.5 :\n",
    "                        x_data = hflip(x_data)\n",
    "                        y_data = hflip(y_data)\n",
    "                elif t == \"T.RandomVerticalFlip(p=1)\" :\n",
    "                    if torch.rand(1)< 0.5 :\n",
    "                        x_data = vflip(x_data)\n",
    "                        y_data = vflip(y_data)\n",
    "                        \n",
    "\n",
    "                elif t == \"T.RandomRotation(degrees=(360 , 360))\" : \n",
    "                    if torch.rand(1)< 0.5 :\n",
    "                        angle = int(torch.rand(1)*360)\n",
    "                        rotate_funct = T.RandomRotation(degrees=(angle, angle))\n",
    "                        x_data = rotate_funct(x_data)\n",
    "                        y_data = rotate_funct(y_data)\n",
    "\n",
    "                else :\n",
    "                    x_data = t(x_data)\n",
    "                    y_data =t(y_data)\n",
    "                \n",
    "        #x_data, y_data = torch.from_numpy(x_data).type(torch.float32), torch.from_numpy(y_data).type(torch.float32)\n",
    "        x_data = torch.permute(x_data , (1, 2 , 0))\n",
    "        y_data = torch.permute(y_data , (1 , 2 , 0))\n",
    "        y_data = y_data[: , : , 2] #edit to the last channel\n",
    "        y_data = np.expand_dims( y_data, axis= -1) #(512 512) -> (512 512 1)\n",
    "        return x_data , y_data  #torch\n",
    "\n",
    "        #torch or numpy to chose\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'30920239', '30268702', '52847319', '13488672', '20483981', '29729120', '29816081', '28267864', '29301889', '25385854', '27237207', '20947834', '26658418', '31001430', '13661917', '15525495', '27710844', '11775010', '21105320', '27103788', '27931321', '29285707', '18765851', '26608196', '27472711', '31046225', '21381973', '29177415', '28248060', '31406726', '19315335', '15176128', '18456622', '17244854', '28358688', '15552568', '14041674', '16113441', '12261283', '27607264', '31445157', '33056266', '24237245', '80190941', '19425238', '22296689', '26082459', '25825974', '26073373', '30732894', '11009708', '30871154', '21372392', '10027124', '26434694', '30517836'}\n",
      "success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'print(len(name_set))\\nprint(len(name_set2))\\nprint(len(non_name_set))\\nprint(len(PR_name_set))'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "non_name_set = set()\n",
    "PR_name_set = set()\n",
    "name_set = set()\n",
    "total_data_path = [non_T1C_data , non_T1_data , non_Flair_data , non_T2_data , PR_T1C_data , PR_T1_data , PR_Flair_data  , PR_T2_data]\n",
    "count1 = 0\n",
    "for folder_name in total_data_path :\n",
    "    for name in os.listdir(folder_name) :\n",
    "        name = name.strip('.jpg').strip('.json')\n",
    "        if name == 'ipynb_checkpoint' :\n",
    "            continue\n",
    "        name_set.add(name)\n",
    "        if count1 <= 3 :\n",
    "            non_name_set.add(name)\n",
    "        else :\n",
    "            PR_name_set.add(name)\n",
    "    count1 += 1\n",
    "#print(name_set)\n",
    "\n",
    "\n",
    "non_name_set2 = set()\n",
    "PR_name_set2 = set()\n",
    "name_set2 = set()\n",
    "count2 = 0\n",
    "total_target_path = [ non_T1C_target , non_T1_target , non_Flair_target  , non_T2_target , PR_T1C_target , PR_T1_target ,  PR_Flair_target , PR_T2_target ]\n",
    "for folder_name2 in total_target_path :\n",
    "    for name in os.listdir(folder_name2):\n",
    "        name = name.strip('.png')\n",
    "        if name == 'ipynb_checkpoint':\n",
    "            continue\n",
    "        name_set2.add(name)\n",
    "        if count2 <= 3 :\n",
    "            non_name_set2.add(name)\n",
    "        else :\n",
    "            PR_name_set2.add(name)\n",
    "    count2 += 1\n",
    "print(name_set2)\n",
    "\n",
    "\n",
    "#same patients set (make sure no mismatch)\n",
    "if name_set2 == name_set :\n",
    "    print('success')\n",
    "\"\"\"print(len(name_set))\n",
    "print(len(name_set2))\n",
    "print(len(non_name_set))\n",
    "print(len(PR_name_set))\"\"\"\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n",
      "x_shape torch.Size([224, 512, 512, 3])\n",
      "y_shape torch.Size([224, 512, 512, 1])\n",
      "x y mean0 tensor(0.0930) tensor(0.0058)\n",
      "x y mean1 tensor(0.0932) tensor(0.0058)\n",
      "x y mean2 tensor(0.0897) tensor(0.0058)\n",
      "x y std0 tensor(0.1633) tensor(0.0538)\n",
      "x y std01 tensor(0.1582) tensor(0.0538)\n",
      "x y std2 tensor(0.1550) tensor(0.0538)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'x y mean0 tensor(0.1291) tensor(0.0205)\\nx y mean1 tensor(0.0905) tensor(0.0205)\\nx y mean2 tensor(0.1105) tensor(0.0205)\\nx y std0 tensor(0.1400) tensor(0.0993)\\nx y std01 tensor(0.1097) tensor(0.0993)\\nx y std2 tensor(0.1298) tensor(0.0993)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#same patient (same index)\n",
    "total_data_list = []\n",
    "total_target_list =[]  \n",
    "for name in non_name_set :\n",
    "    name1 = name + \".jpg\"\n",
    "    name2 = name + \".png\"\n",
    "    total_data_list.append(os.path.join(non_T1C_data , name1))\n",
    "    total_data_list.append(os.path.join(non_T1_data , name1))\n",
    "    total_data_list.append(os.path.join(non_Flair_data , name1))\n",
    "    total_data_list.append(os.path.join(non_T2_data , name1))\n",
    "    total_target_list.append(os.path.join(non_T1C_target , name2))\n",
    "    total_target_list.append(os.path.join(non_T1_target , name2))\n",
    "    total_target_list.append(os.path.join(non_Flair_target , name2))\n",
    "    total_target_list.append(os.path.join(non_T2_target , name2)) \n",
    "\n",
    "for name in PR_name_set :\n",
    "    name1 = name + \".jpg\"\n",
    "    name2 = name + \".png\"\n",
    "    total_data_list.append(os.path.join(PR_T1C_data , name1))\n",
    "    total_data_list.append(os.path.join(PR_T1_data , name1))\n",
    "    total_data_list.append(os.path.join(PR_Flair_data , name1))\n",
    "    total_data_list.append(os.path.join(PR_T2_data , name1))\n",
    "    total_target_list.append(os.path.join(PR_T1C_target , name2))\n",
    "    total_target_list.append(os.path.join(PR_T1_target , name2))\n",
    "    total_target_list.append(os.path.join(PR_Flair_target , name2))\n",
    "    total_target_list.append(os.path.join(PR_T2_target , name2))   \n",
    "\n",
    "train_data_list = total_data_list[0 : int(len(total_data_list)*0.8)] #80%data\n",
    "train_target_list =  total_target_list[0 : int(len(total_target_list)*0.8) ] #80%target\n",
    "\n",
    "test_data_list = total_data_list[int(len(total_data_list)*0.8) : len(total_data_list)] #20%data\n",
    "test_target_list = total_target_list[int(len(total_target_list)*0.8) : len(total_target_list)] #20%target\n",
    "\n",
    "#initial img visualize\n",
    "\"\"\"count , count2 = 0 , 0\n",
    "for img1 in total_data_list:\n",
    "    img1 = cv2.imread(img1)\n",
    "    print(type(img1))\n",
    "    plt.imshow(img1)\n",
    "    plt.show()\n",
    "    print(img1.shape)\n",
    "    count += 1\n",
    "    if count > 5 : break\n",
    "for img2 in total_target_list:\n",
    "    img2 = cv2.imread(img2)\n",
    "    print(type(img2))\n",
    "    plt.imshow(img2)\n",
    "    plt.show()\n",
    "    print(img2.shape)\n",
    "    count2 += 1\n",
    "    if count2 > 5: break\"\"\"\n",
    "\n",
    "\n",
    "#std and mean calculated\n",
    "#total dataset\n",
    "total_dataset = Total_dataset(total_data_list ,\n",
    "                              total_target_list,\n",
    "                              transform= [T.ToTensor()]\n",
    ")\n",
    "print(len(total_dataset))\n",
    "total_dataloader = DataLoader(total_dataset , batch_size= len(total_dataset) , shuffle= False)\n",
    "#total_dataloader = DataLoader(total_dataset , batch_size= len(total_data_list) , shuffle= False)\n",
    "\n",
    "x , y = next(iter(total_dataloader))\n",
    "print(\"x_shape\", x.shape)\n",
    "print(\"y_shape\" , y.shape)\n",
    "\n",
    "\n",
    "#img visualize after to_tensor\n",
    "\"\"\"for index in range(5) :\n",
    "    #cv2.imshow(f\"{index}.jpg\" , x[index , : , : , :].detach().cpu().numpy())\n",
    "    #cv2.imshow(f\"{index}.png\" , y[index , : , : , : ].detach().cpu().numpy())\n",
    "    plt.imshow(x[index , : , : , :])\n",
    "    plt.show()\n",
    "    plt.imshow(y[index , : , : , :])\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(\"x y mean0\" ,x[0].mean() , y[0].mean()) #ch1\n",
    "print(\"x y mean1\" ,x[1].mean() , y[1].mean()) #ch2\n",
    "print(\"x y mean2\" ,x[2].mean() , y[2].mean()) #ch3\n",
    "print(\"x y std0\" , x[0].std() , y[0].std()) #ch1\n",
    "print(\"x y std01\" , x[1].std() , y[1].std()) #ch2\n",
    "print(\"x y std2\" , x[2].std() , y[2].std() ) #ch3 \n",
    "\n",
    "\"\"\"x y mean0 tensor(0.1291) tensor(0.0205)\n",
    "x y mean1 tensor(0.0905) tensor(0.0205)\n",
    "x y mean2 tensor(0.1105) tensor(0.0205)\n",
    "x y std0 tensor(0.1400) tensor(0.0993)\n",
    "x y std01 tensor(0.1097) tensor(0.0993)\n",
    "x y std2 tensor(0.1298) tensor(0.0993)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data augmentation\n",
    "可能問題 :\n",
    "    random transform data_x data_y 沒有對到(機率)\n",
    "    \n",
    "    目前label 全黑 (solve : take the last channel)\n",
    "    permute 可能有問題 \n",
    "    label根本沒對到tumor位置 (1 , 2 , 0) (2 , 1 , 0)不知道為啥整個圖轉掉\n",
    "    \n",
    "    為啥mean 跟 standard每次跑數字都不同???????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "torch.Size([300, 512, 512, 3])\n",
      "torch.Size([300, 512, 512, 1])\n",
      "train_x = shape: torch.Size([30, 512, 512, 3]); type: torch.float32\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "train_y = shape: torch.Size([30, 512, 512, 1]); type: torch.float32\n",
      "train_x = min: 0.0; max: 1.0\n",
      "train_y = shape: torch.Size([30, 512, 512, 1]); class: tensor([0.0000, 0.0078, 0.0156, 0.0221, 0.0234, 0.0312, 0.0391, 0.0469, 0.0547,\n",
      "        0.0625, 0.0703, 0.0781, 0.0859, 0.0938, 0.1016, 0.1094, 0.1172, 0.1250,\n",
      "        0.1328, 0.1406, 0.1484, 0.1562, 0.1641, 0.1719, 0.1797, 0.1875, 0.1953,\n",
      "        0.2031, 0.2109, 0.2188, 0.2266, 0.2344, 0.2422, 0.2500, 0.2578, 0.2656,\n",
      "        0.2734, 0.2813, 0.2891, 0.2969, 0.3047, 0.3125, 0.3203, 0.3281, 0.3359,\n",
      "        0.3438, 0.3516, 0.3594, 0.3672, 0.3750, 0.3828, 0.3906, 0.3984, 0.4062,\n",
      "        0.4141, 0.4219, 0.4297, 0.4375, 0.4453, 0.4531, 0.4609, 0.4688, 0.4766,\n",
      "        0.4844, 0.4922, 0.5000, 0.5078, 0.5156, 0.5234, 0.5312, 0.5391, 0.5469,\n",
      "        0.5547, 0.5625, 0.5703, 0.5781, 0.5859, 0.5938, 0.6016, 0.6094, 0.6172,\n",
      "        0.6250, 0.6328, 0.6406, 0.6484, 0.6562, 0.6641, 0.6719, 0.6797, 0.6875,\n",
      "        0.6953, 0.7031, 0.7109, 0.7188, 0.7266, 0.7344, 0.7422, 0.7500, 0.7578,\n",
      "        0.7656, 0.7734, 0.7813, 0.7891, 0.7969, 0.8047, 0.8125, 0.8203, 0.8281,\n",
      "        0.8359, 0.8438, 0.8516, 0.8594, 0.8672, 0.8750, 0.8828, 0.8906, 0.8984,\n",
      "        0.9062, 0.9141, 0.9219, 0.9297, 0.9375, 0.9453, 0.9531, 0.9609, 0.9688,\n",
      "        0.9766, 0.9844, 0.9922, 1.0000]); type: torch.float32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#目前尚未 rescale 跟 normalize\n",
    "print(len(train_data_list))\n",
    "#rename transform\n",
    "normal = \"T.Normalize([1 , 1 , 1] , [1 , 1 , 1])\"\n",
    "v_flip = \"T.RandomVerticalFlip(p=1)\"\n",
    "h_flip = \"T.RandomHorizontalFlip(p=1)\"\n",
    "r_rotate = \"T.RandomRotation(degrees=(360 , 360))\"\n",
    "#note : rewrite every probability functions in Total_dataset()\n",
    "train_dataset = Total_dataset(train_data_list,\n",
    "                             train_target_list ,\n",
    "                             transform= [T.ToTensor() , normal , \n",
    "                             v_flip , \n",
    "                             h_flip , \n",
    "                             r_rotate,\n",
    "                             T.RandomAutocontrast(p=1) ,\n",
    "                             ])\n",
    "                             \n",
    "#transform should add                \n",
    "training_dataloader = DataLoader(train_dataset,\n",
    "                                 batch_size=30,\n",
    "                                 shuffle=True)\n",
    "\n",
    "test_dataset = Total_dataset(test_data_list ,\n",
    "                            test_target_list ,\n",
    "                            transform= [ T.ToTensor() , T.Normalize([1 , 1 , 1] , [1 , 1 , 1]) , T.RandomAutocontrast(p=1)])\n",
    "\n",
    "testing_dataloader = DataLoader(test_dataset,\n",
    "                                 batch_size=30,\n",
    "                                 shuffle=True)\n",
    "\n",
    "\n",
    "\"\"\"change to keras accepted dtype\"\"\"\n",
    "#dtype (torch.float32) torch.size(batchsize , 3 , 512 , 512)\n",
    "#take total 170 imgs (17 * 10)\n",
    "\n",
    "\n",
    "train_x, train_y = next(iter(training_dataloader))\n",
    "\"\"\"for index in range(50) :\n",
    "    plt.imshow(train_x[index , : , : , :])\n",
    "    plt.show()\n",
    "    plt.imshow(train_y[index , : , : , :])\n",
    "    plt.show()\"\"\"\n",
    "\n",
    "total_train_img = train_x\n",
    "total_train_lbl = train_y\n",
    "batch_count = 9\n",
    "for i in range(batch_count) :\n",
    "    train_x, train_y = next(iter(training_dataloader))\n",
    "    total_train_img = torch.cat((total_train_img , train_x) , dim= 0)\n",
    "    total_train_lbl = torch.cat((total_train_lbl , train_y) , dim = 0)\n",
    "print(total_train_img.shape)\n",
    "print(total_train_lbl.shape)\n",
    "    \n",
    "test_x , test_y = next(iter(testing_dataloader))\n",
    "\n",
    "\n",
    "#all img and label size is (512 , 512 , 3)\n",
    "print(f'train_x = shape: {train_x.shape}; type: {train_x.dtype}')\n",
    "print(train_x[0])\n",
    "print(f'train_y = shape: {train_y.shape}; type: {train_y.dtype}')\n",
    "print(f'train_x = min: {train_x.min()}; max: {train_x.max()}')\n",
    "print(f'train_y = shape: {train_y.shape}; class: {train_y.unique()}; type: {train_y.dtype}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 512, 512, 3)\n",
      "(300, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "np_total_train_img = total_train_img.detach().cpu().numpy()\n",
    "np_total_train_lbl = total_train_lbl.detach().cpu().numpy()\n",
    "print(np_total_train_img.shape)\n",
    "print(np_total_train_lbl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend as keras\n",
    "\n",
    "#layer visualized\n",
    "import tensorflow as tf\n",
    "\"\"\"input_shape = (1 ,512, 512, 3)\n",
    "x = tf.random.normal(input_shape)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def unet(pretrained_weights = None,input_size = (512 , 512 , 3)):\n",
    "    #Dropout(rate = 0.5) -> rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "    #->  as to prevent from overfitting\n",
    "    \n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    #pool1 -> (_ , 256 , 256 , 64)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    #pool2 -> (_ , 128 , 128 , 128)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    #pool3 -> (_ , 64 , 64 , 256)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    #pool4 -> (_ , 32 , 32 , 512)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    #drop5 -> (_ ,  32, 32, 1024)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    #up6 (1, 64, 64, 512)\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    #conv6 ->(1, 64, 64, 512)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    #up7 (1, 128, 128, 256)\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    #conv7 (1, 128, 128, 256)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    #up8 (1, 256, 256, 128)\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    #up9 (1, 512, 512, 64)\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    #merge9 (1, 512, 512, 128)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding =  'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs, conv10)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    #model.summary()\n",
    "\n",
    "    if(pretrained_weights):\n",
    "    \tmodel.load_weights(pretrained_weights)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "#batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is fitting.......\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " failed to allocate memory\n\t [[node binary_crossentropy/logistic_loss/mul\n (defined at C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\backend.py:5158)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_2908]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node binary_crossentropy/logistic_loss/mul:\nIn[0] model/conv2d_23/BiasAdd (defined at C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\layers\\convolutional.py:265)\t\nIn[1] IteratorGetNext (defined at C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\engine\\training.py:866)\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\asyncio\\events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2902, in run_cell\n>>>     raw_cell, store_history, silent, shell_futures)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3173, in run_cell_async\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_19452/2824250161.py\", line 6, in <module>\n>>>     model.fit(x = np_total_train_img , y = np_total_train_lbl , batch_size = 2 , epochs= 10 , steps_per_epoch = 5 , callbacks=[model_checkpoint])\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\engine\\training.py\", line 810, in train_step\n>>>     y, y_pred, sample_weight, regularization_losses=self.losses)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n>>>     loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n>>>     losses = call_fn(y_true, y_pred)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\losses.py\", line 245, in call\n>>>     return ag_fn(y_true, y_pred, **self._fn_kwargs)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\losses.py\", line 1807, in binary_crossentropy\n>>>     backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\backend.py\", line 5158, in binary_crossentropy\n>>>     return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\user\\AppData\\Local\\Temp/ipykernel_19452/2824250161.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model is fitting.......\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#model.fit_generator(training_dataloader,steps_per_epoch=300,epochs=1,callbacks=[model_checkpoint])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_total_train_img\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_total_train_lbl\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 59\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  failed to allocate memory\n\t [[node binary_crossentropy/logistic_loss/mul\n (defined at C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\backend.py:5158)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_2908]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node binary_crossentropy/logistic_loss/mul:\nIn[0] model/conv2d_23/BiasAdd (defined at C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\layers\\convolutional.py:265)\t\nIn[1] IteratorGetNext (defined at C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\engine\\training.py:866)\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\asyncio\\events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2902, in run_cell\n>>>     raw_cell, store_history, silent, shell_futures)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3173, in run_cell_async\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_19452/2824250161.py\", line 6, in <module>\n>>>     model.fit(x = np_total_train_img , y = np_total_train_lbl , batch_size = 2 , epochs= 10 , steps_per_epoch = 5 , callbacks=[model_checkpoint])\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\engine\\training.py\", line 810, in train_step\n>>>     y, y_pred, sample_weight, regularization_losses=self.losses)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n>>>     loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n>>>     losses = call_fn(y_true, y_pred)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\losses.py\", line 245, in call\n>>>     return ag_fn(y_true, y_pred, **self._fn_kwargs)\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\losses.py\", line 1807, in binary_crossentropy\n>>>     backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n>>> \n>>>   File \"C:\\Users\\user\\anaconda3\\envs\\2dunet\\lib\\site-packages\\keras\\backend.py\", line 5158, in binary_crossentropy\n>>>     return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n>>> "
     ]
    }
   ],
   "source": [
    "\"\"\"training and saving\"\"\"\n",
    "model = unet()\n",
    "model_checkpoint = ModelCheckpoint('CMtumor_unet1.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "print(\"model is fitting.......\")\n",
    "#model.fit_generator(training_dataloader,steps_per_epoch=300,epochs=1,callbacks=[model_checkpoint])\n",
    "model.fit(x = np_total_train_img , y = np_total_train_lbl , batch_size = 2 , epochs= 10 , steps_per_epoch = 5 , callbacks=[model_checkpoint])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b562ed37188020c8a8c07b69b9eae1a2662f56daf78cc992f9676fd3c0f765c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('2dunet': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
