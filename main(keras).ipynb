{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skimage.transform as trans\n",
    "import skimage.io as io\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np \n",
    "import os\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "seed_value= 666\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preporcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"load data\"\"\"\n",
    "from cfg import *\n",
    "#patients in data_folcder is cinsistent with traget_folder\n",
    "non_name_set = set()\n",
    "PR_name_set = set()\n",
    "name_set = set()\n",
    "total_data_path = [non_T1C_data , non_T1_data , non_Flair_data , non_T2_data , PR_T1C_data , PR_T1_data , PR_Flair_data  , PR_T2_data]\n",
    "count1 = 0\n",
    "for folder_name in total_data_path :\n",
    "    for name in os.listdir(folder_name) :\n",
    "        name = name.strip('.jpg').strip('.json')\n",
    "        if name == 'ipynb_checkpoint' :\n",
    "            continue\n",
    "        name_set.add(name)\n",
    "        if count1 <= 3 :\n",
    "            non_name_set.add(name)\n",
    "        else :\n",
    "            PR_name_set.add(name)\n",
    "    count1 += 1\n",
    "#print(name_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_list = []\n",
    "total_target_list =[] \n",
    "for non_name , pr_name in zip(non_name_set , PR_name_set) :\n",
    "    non_name1 = non_name + \".jpg\"\n",
    "    non_name2 = non_name + \".png\"\n",
    "    pr_name1 = pr_name + \".jpg\"\n",
    "    pr_name2 = pr_name + \".png\"\n",
    "    total_data_list.append(os.path.join(non_T1C_data , non_name1))\n",
    "    total_data_list.append(os.path.join(non_T1_data , non_name1))\n",
    "    total_data_list.append(os.path.join(non_Flair_data , non_name1))\n",
    "    total_data_list.append(os.path.join(non_T2_data , non_name1))\n",
    "    total_target_list.append(os.path.join(non_T1C_target , non_name2))\n",
    "    total_target_list.append(os.path.join(non_T1_target , non_name2))\n",
    "    total_target_list.append(os.path.join(non_Flair_target , non_name2))\n",
    "    total_target_list.append(os.path.join(non_T2_target , non_name2))\n",
    "    \n",
    "    total_data_list.append(os.path.join(PR_T1C_data , pr_name1))\n",
    "    total_data_list.append(os.path.join(PR_T1_data , pr_name1))\n",
    "    total_data_list.append(os.path.join(PR_Flair_data , pr_name1))\n",
    "    total_data_list.append(os.path.join(PR_T2_data , pr_name1))\n",
    "    total_target_list.append(os.path.join(PR_T1C_target , pr_name2))\n",
    "    total_target_list.append(os.path.join(PR_T1_target , pr_name2))\n",
    "    total_target_list.append(os.path.join(PR_Flair_target , pr_name2))\n",
    "    total_target_list.append(os.path.join(PR_T2_target , pr_name2))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "75\n",
      "11\n",
      "11\n",
      "18\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "train_data_list = total_data_list[0 : int(len(total_data_list)*0.9)] #90%data\n",
    "train_target_list =  total_target_list[0 : int(len(total_target_list)*0.9) ] #90%target\n",
    "\n",
    "test_data_list = total_data_list[int(len(total_data_list)*0.9) : len(total_data_list)] #90%data\n",
    "test_target_list = total_target_list[int(len(total_target_list)*0.9) : len(total_target_list)] #90%target\n",
    "\n",
    "val_img = train_data_list[0 : int(0.2* len(train_data_list))]\n",
    "val_lbl = train_target_list[0 : int(0.2* len(train_target_list))]\n",
    "train_data_list = train_data_list[int(0.2* len(train_data_list)) : ]\n",
    "train_target_list = train_target_list[int(0.2* len(train_target_list)) : ]\n",
    "print(len(train_data_list))\n",
    "print(len(train_target_list))\n",
    "print(len(test_data_list))\n",
    "print(len(test_target_list))\n",
    "print(len(val_img))\n",
    "print(len(val_img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.preprocessing.image.NumpyArrayIterator object at 0x000001CD83990748>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"test\"\"\"\n",
    "import cv2\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "input1 = cv2.imread(train_data_list[0])\n",
    "input1 = np.expand_dims(input1 , axis= 0 )\n",
    "train_iterator = datagen.flow(input1, batch_size=1)\n",
    "print(train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "#mask[mask > 0.5] = 1\n",
    "#mask[mask <= 0.5] = 0\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomFlip(\"vertical\"),\n",
    "        layers.RandomZoom(0.4),\n",
    "        layers.RandomRotation(0.3),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#(train_mode = False) for (test and val)\n",
    "def train_gen(x_path_list , y_path_list , batch_size , train_mode = True , shuffle = True):\n",
    "    data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomFlip(\"vertical\"),\n",
    "        layers.RandomZoom(0.4),\n",
    "        layers.RandomRotation(0.3),\n",
    "    ]\n",
    "    )\n",
    "    for x , y in zip(x_path_list , y_path_list) :\n",
    "        x , y = cv2.imread(x) , cv2.imread(y)\n",
    "        if x.shape != (512 , 512 , 3) :\n",
    "            x = cv2.resize(x , (512 , 512))\n",
    "        if y.shape != (512 , 512 , 3) :\n",
    "            y = cv2.resize(y , (512 ,512))\n",
    "        x = layers.Rescaling(1.0 / 255.0)(x)\n",
    "        y = layers.Rescaling(1.0 / 255.0)(y)\n",
    "        #do without normalization first\n",
    "        #x = layers.BatchNormalization()\n",
    "        if train_mode :\n",
    "            data_loader = ImageDataGenerator(data_augmentation)\n",
    "            data_loader.flow()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512, 512, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndef unet(pretrained_weights = None,input_size = (512 , 512 , 3)):\\n\\n    #Dropout(rate = 0.5) -> rate: Float between 0 and 1. Fraction of the input units to drop.\\n    #->  as to prevent from overfitting\\n    \\n    inputs = Input(input_size)\\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\\n    #pool1 -> (_ , 256 , 256 , 64)\\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\\n    #pool2 -> (_ , 128 , 128 , 128)\\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\\n    #pool3 -> (_ , 64 , 64 , 256)\\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\\n    drop4 = Dropout(0.5)(conv4)\\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\\n    #pool4 -> (_ , 32 , 32 , 512)\\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\\n    drop5 = Dropout(0.5)(conv5)\\n    #drop5 -> (_ ,  32, 32, 1024)\\n\\n    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\\n    #up6 (1, 64, 64, 512)\\n    merge6 = concatenate([drop4,up6], axis = 3)\\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\\n    #conv6 ->(1, 64, 64, 512)\\n\\n    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\\n    #up7 (1, 128, 128, 256)\\n    merge7 = concatenate([conv3,up7], axis = 3)\\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\\n    #conv7 (1, 128, 128, 256)\\n\\n    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\\n    #up8 (1, 256, 256, 128)\\n    merge8 = concatenate([conv2,up8], axis = 3)\\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\\n\\n    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\\n    #up9 (1, 512, 512, 64)\\n    merge9 = concatenate([conv1,up9], axis = 3)\\n    #merge9 (1, 512, 512, 128)\\n    conv9 = Conv2D(64, 3, activation = 'relu', padding =  'same', kernel_initializer = 'he_normal')(merge9)\\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\\n    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\\n    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\\n    #conv10 ()\\n\\n    model = Model(inputs, conv10)\\n\\n    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\\n    \\n    #model.summary()\\n\\n    if(pretrained_weights):\\n    \\tmodel.load_weights(pretrained_weights)\\n\\n    return model\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend as keras\n",
    "import tensorflow as tf\n",
    "\"\"\"\n",
    "input_shape = (1 ,512, 512, 3)\n",
    "inputs = tf.random.normal(input_shape)\n",
    "\"\"\"\n",
    "def unet(pretrained_weights = None,input_size = (512 , 512 , 3)):\n",
    "\n",
    "    #Dropout(rate = 0.5) -> rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "    #->  as to prevent from overfitting\n",
    "    \n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    #pool1 -> (_ , 256 , 256 , 64)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    #pool2 -> (_ , 128 , 128 , 128)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    #pool3 -> (_ , 64 , 64 , 256)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    #pool4 -> (_ , 32 , 32 , 512)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    #drop5 -> (_ ,  32, 32, 1024)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    #up6 (1, 64, 64, 512)\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    #conv6 ->(1, 64, 64, 512)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    #up7 (1, 128, 128, 256)\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    #conv7 (1, 128, 128, 256)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    #up8 (1, 256, 256, 128)\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    #up9 (1, 512, 512, 64)\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    #merge9 (1, 512, 512, 128)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding =  'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "    #conv10 (1, 512, 512, 1)\n",
    "\n",
    "    model = Model(inputs, conv10)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    #model.summary()\n",
    "\n",
    "    if(pretrained_weights):\n",
    "    \tmodel.load_weights(pretrained_weights)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b562ed37188020c8a8c07b69b9eae1a2662f56daf78cc992f9676fd3c0f765c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('2dunet': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
